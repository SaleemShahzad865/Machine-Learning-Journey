# -*- coding: utf-8 -*-
"""Gredient Descent from Scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x7LyagZroUrb75_t8b5Xut1vhlzSHVA_
"""

from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt

X,y=make_regression(n_samples=4,n_features=1,n_informative=1,n_targets=1,noise=80,random_state=13)

plt.scatter(X,y)

"""# Applying Ordinary least square method

"""

from sklearn.linear_model import LinearRegression

reg=LinearRegression()

reg.fit(X,y)

reg.coef_

reg.intercept_

plt.scatter(X,y)
plt.plot(X,reg.predict(X),color='red')

"""# Applying Gredient Descent

lets assume m=78.85 and starting value for b=0
"""

y_pred=((78.35 * X)+0).reshape(4)

plt.scatter(X,y)
plt.plot(X,reg.predict(X),color='red',label='OLS')
plt.plot(X,y_pred,color='#00a65a',label='b=0')
plt.legend()
plt.show()

m=78.35
b=0
loss_slope=-2*np.sum(y-m*X.ravel()-b)
loss_slope

# lets start with a learning rate
lr=0.1
step_size=loss_slope*lr
step_size

b=b-step_size
b

y_pred1=((78.35 * X)+b1).reshape(4)
plt.scatter(X,y)
plt.plot(X,reg.predict(X),color='red',label='OLS')
plt.plot(X,y_pred1,color='#00a65a',label=b)
plt.plot(X,y_pred,color='#A3E407',label='b=0')
plt.legend()
plt.show()

# Interation 2
loss_slope=-2*np.sum(y-m*X.ravel()-b)
print("loss slope:",loss_slope)
lr=0.1
step_size=loss_slope*lr
print("step size:",step_size)
b=b-step_size
print("b:",b)

y_pred2=((78.35 * X)+b).reshape(4)
plt.scatter(X,y)
plt.plot(X,reg.predict(X),color='red',label='Best_fit Line form OLS Method')
plt.plot(X,y_pred2,color='blue',label="Iteration=2")
plt.plot(X,y_pred1,color='#00a65a',label="Iteration=1")
plt.plot(X,y_pred,color='#A3E407',label='Initial b=0')
plt.legend()
plt.show()

# Interation 3
loss_slope=-2*np.sum(y-m*X.ravel()-b)
print("loss slope:",loss_slope)
lr=0.1
step_size=loss_slope*lr
print("step size:",step_size)
b=b-step_size
print("b:",b)

y_pred3=((78.35 * X)+b).reshape(4)
plt.scatter(X,y)
plt.plot(X,reg.predict(X),color='red',label='Best_fit Line form OLS Method')
plt.plot(X,y_pred3,color='#ff5733',label="Iteration=3")
plt.plot(X,y_pred2,color='blue',label="Iteration=2")
plt.plot(X,y_pred1,color='#00a65a',label="Iteration=1")
plt.plot(X,y_pred,color='#A3E407',label='Initial b=0')
plt.legend()
plt.show()

# Performing the Same Thing with the help of loop
# You can change the learning rate lr and vaue of intercept b in the variables for better understanding
b=0
m=78.85
lr=0.1
iteration=3
for i in range(iteration):
  loss_slope=-2*np.sum(y-m*X.ravel()-b)
  step_size=lr*loss_slope
  b=b-step_size
  y_pred=m*X+b
  plt.plot(X,y_pred)
plt.scatter(X,y)

"""# Creating a Gredient Decent Regressor Class


"""

from sklearn.datasets import make_regression
import numpy as np
import matplotlib.pyplot as plt

X,y=make_regression(n_samples=100,n_features=1,n_informative=1,n_targets=1,noise=20)

plt.scatter(X,y)

from sklearn.linear_model import LinearRegression

lr=LinearRegression()

lr.fit(X,y)
print(lr.coef_)
print(lr.intercept_)

class GDRegressor:
  def __init__(self,learning_rate,epochs):
    self.lr=learning_rate
    self.epochs=epochs
    self.m=100
    self.b=-120
  def fit(self,X,y):
    for i in range(self.epochs):
      # Calcuting b and m using Gredient Descent Method
      loss_slope_b=-2*np.sum(y-self.m*X.ravel()-self.b)
      loss_slope_m=-2*np.sum((y-self.m*X.ravel()-self.b)* X.ravel())
      step_size_b=self.lr*loss_slope_b
      step_size_m=self.lr*loss_slope_m
      self.b=self.b-step_size_b
      self.m=self.m-step_size_m
    print(self.b,self.m)

gd=GDRegressor(0.001,100)

gd.fit(X,y)